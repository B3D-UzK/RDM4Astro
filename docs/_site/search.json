[
  {
    "objectID": "DataOwnership.html",
    "href": "DataOwnership.html",
    "title": "RDM 4 Astro",
    "section": "",
    "text": "Data ownership in astronomical projects is a crucial aspect that dictates who has the legal rights and control over the collected and generated data. This ownership is typically not vested in the individual researcher but is instead held by the funding agency that provides the resources for the project or the institution where the researcher is employed.\nThe funding agency or the institution usually has the ultimate authority over the data, given that they provide the financial and infrastructural support necessary for the data collection and analysis. This includes the provision of equipment, software, and other resources used in the data collection process. They also often set the policies and guidelines for data management, including storage, access, and sharing protocols.\nThe researcher, on the other hand, often acts as a custodian of the data. In this role, they are responsible for the day-to-day management of the data, ensuring its quality, integrity, and security. They are also typically responsible for ensuring that the data is used in accordance with the policies set by the funding agency or institution. This includes adhering to any restrictions on data sharing or publication, as well as ensuring that any use of the data complies with ethical guidelines and legal requirements.\nHowever, the specifics of data ownership can vary depending on the terms of the funding agreement and the policies of the institution. Therefore, it is crucial for researchers involved in astronomical projects to be fully aware of these terms and policies. They should familiarize themselves with any data management plans or guidelines provided by their funding agency or institution, and ensure that they adhere to these throughout the course of their research."
  },
  {
    "objectID": "SharingData.html",
    "href": "SharingData.html",
    "title": "RDM 4 Astro",
    "section": "",
    "text": "Benefits of data sharing:\n\nReinforces open scientific inquiry\nSupports verification and replication of original results\nPromotes new research and testing of alternative methods\nEncourages collaboration and multiple perspectives\nProvides important teaching resources\nReduces costs by avoiding duplicate data collection efforts\nProtects against faulty or fraudulent data\nEnhances visibility and overall impact of research projects\nPreserves data for future use\nHelps the broader community and individual researchers do better research\n\nKey players in data sharing are the data creator/producer, secondary data user, and data repository\nData repository plays a key role in enhancing discovery and reuse of data and creating formal data citation\nSharing data is encouraged by funders and required in some cases\nData sharing benefits both the broader research community and individual researchers.\n\n\n\n\nChallenges to data sharing:\n\nMaking data shareable takes time and effort\nPerceived risks from loss of control of the data\nData contained confidential or sensitive information\nOwnership of the data may be unclear or problematic\nLack of incentives for sharing data\n\nAdditional challenge: lack of experience and knowledge of data management\nResearchers and information professionals can overcome these challenges by applying data management practices.\n\n\n\n\nLack of career incentives is an obstacle to sharing data.\nData citation provides a standardized method for citing data, and can be used to reward researchers for sharing their data.\nThe Joint Declaration of Data Citation Principles establishes principles for data citation, including credit and attribution, evidence, unique identification, access, persistence, specificity and verifiability, and interoperability and flexibility.\nDataCite is a group that works with data repositories to assign persistent identifiers such as DOIs to data, supporting simple and effective methods of data citation, discovery, and access.\nProper citation of datasets supports reproducibility of research, ensures proper credit for researchers, and enables tracking of data reuse.\nResearchers should consider whether the repository they choose supports the creation of unique data citations that embody the Joint Declaration of Data Citation Principles."
  },
  {
    "objectID": "SharingData.html#sharing-foundations",
    "href": "SharingData.html#sharing-foundations",
    "title": "RDM 4 Astro",
    "section": "",
    "text": "Benefits of data sharing:\n\nReinforces open scientific inquiry\nSupports verification and replication of original results\nPromotes new research and testing of alternative methods\nEncourages collaboration and multiple perspectives\nProvides important teaching resources\nReduces costs by avoiding duplicate data collection efforts\nProtects against faulty or fraudulent data\nEnhances visibility and overall impact of research projects\nPreserves data for future use\nHelps the broader community and individual researchers do better research\n\nKey players in data sharing are the data creator/producer, secondary data user, and data repository\nData repository plays a key role in enhancing discovery and reuse of data and creating formal data citation\nSharing data is encouraged by funders and required in some cases\nData sharing benefits both the broader research community and individual researchers.\n\n\n\n\nChallenges to data sharing:\n\nMaking data shareable takes time and effort\nPerceived risks from loss of control of the data\nData contained confidential or sensitive information\nOwnership of the data may be unclear or problematic\nLack of incentives for sharing data\n\nAdditional challenge: lack of experience and knowledge of data management\nResearchers and information professionals can overcome these challenges by applying data management practices.\n\n\n\n\nLack of career incentives is an obstacle to sharing data.\nData citation provides a standardized method for citing data, and can be used to reward researchers for sharing their data.\nThe Joint Declaration of Data Citation Principles establishes principles for data citation, including credit and attribution, evidence, unique identification, access, persistence, specificity and verifiability, and interoperability and flexibility.\nDataCite is a group that works with data repositories to assign persistent identifiers such as DOIs to data, supporting simple and effective methods of data citation, discovery, and access.\nProper citation of datasets supports reproducibility of research, ensures proper credit for researchers, and enables tracking of data reuse.\nResearchers should consider whether the repository they choose supports the creation of unique data citations that embody the Joint Declaration of Data Citation Principles."
  },
  {
    "objectID": "SharingData.html#enabling-sharing",
    "href": "SharingData.html#enabling-sharing",
    "title": "RDM 4 Astro",
    "section": "Enabling Sharing",
    "text": "Enabling Sharing\n\nProtecting Confidentiality\n\nResearchers have an ethical obligation to protect the privacy of study participants when collecting data that deals with human subjects.\nConfidentiality breaches can have serious consequences, including negative impacts on a researcher’s career and institution, as well as legal sanctions.\nProtecting confidentiality requires careful consideration and special handling of not only direct identifiers, but also indirect identifiers.\nAnonymizing qualitative data is best done using a pre-planned anonymization scheme that modifies the qualitative dataset to protect respondent confidentiality.\nThere are other strategies currently being developed to protect confidential data, but the most common strategy remains to anonymize data by removing variables, applying statistical techniques, or redacting information to lessen the potential for identifying an individual.\nResearchers should consider future research questions and carefully consider a variable’s analytic importance to determine the best strategy for anonymizing data to maximize usability, and may consult their institutional review board, statistical experts, or information professionals.\n\n\n\nIntellectual Property and Data Ownership\n\nData ownership and intellectual property rights can complicate sharing research data.\nIntellectual property rights apply to any work created or invented with intellectual effort.\nDifferent forms of research data can have different intellectual property rights and legal jurisdictions.\nResearchers should resolve any data ownership issues before sharing data.\nData ownership can be complicated due to multiple stakeholders and collaborations.\nResearchers should come to an agreement on data usage and ownership at the beginning of a project.\nInstitutional policies and funder policies can influence data ownership and sharing.\nResearchers may need permission from data producers to share proprietary data.\nInformation and legal professionals can assist researchers in determining policies that affect data ownership.\n\n\n\nAccess\n\nSharing research data has benefits and challenges.\nAccess to data may vary based on external limitations or researchers’ needs.\nData ownership issues or data containing sensitive or confidential information may affect how and where a researcher provides access.\nThree types of restrictions that are commonly placed on data: embargos, technological access restrictions, and data use agreements.\nEmbargos are a specified period of time when access to data will be restricted.\nTechnological access restrictions may require users to log in to a particular system and authenticate the relationship to an institution to access certain data.\nData use agreements explicitly outline an agreement between the data producer and secondary data user.\nIdeally, researchers should make data as open as possible, but this may not always be feasible.\nApplying a standard Creative Commons license can positively impact the potential for reuse.\nCreative Commons provides a robust legal code, a human-readable summary, and a machine-readable layer of code that can help make resources interoperable across systems.\nFive main Creative Commons license categories: Attribution, NonCommercial, No Derivative Works, ShareAlike, and CC0.\nResearchers should explain restrictions within the terms of use and apply a standard license to enable informed reuse."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nAstrophysics, the study of the universe, relies on vast amounts of data collected from observatories, telescopes, and space missions. Efficient research data management is crucial for extracting insights and advancing our understanding of the cosmos. This involves acquiring, storing, curating, and standardizing data, as well as utilizing advanced computational techniques. Additionally, data preservation and open access principles ensure long-term accessibility and collaboration. In this rapidly advancing field, effective research data management is essential for unraveling the mysteries of the universe."
  },
  {
    "objectID": "FAIR Astro.html",
    "href": "FAIR Astro.html",
    "title": "RDM 4 Astro",
    "section": "",
    "text": "The FAIR principles (Findable, Accessible, Interoperable, Reusable) are guidelines designed to improve the Findability, Accessibility, Interoperability, and Reuse of digital assets, particularly scientific data. These principles emphasize machine-actionability, which is the capacity of computational systems to find, access, interoperate, and reuse data with minimal human intervention."
  },
  {
    "objectID": "FAIR Astro.html#laboratory",
    "href": "FAIR Astro.html#laboratory",
    "title": "RDM 4 Astro",
    "section": "Laboratory",
    "text": "Laboratory\nIn the context of laboratory astrophysics, the FAIR principles can be applied to manage research data using Electronic Lab Notebooks (ELNs). ELNs are digital platforms that play a crucial role in planning, conducting, and analyzing scientific experiments. They offer significant advantages over traditional paper-based lab books, such as direct linkage to laboratory devices, analysis software, or storage systems, making results more reproducible and research more efficient.\nHere’s how the FAIR principles can be applied in the context of ELNs:\n\nFindable: The first step in (re)using data is to find them. Metadata and data should be easy to find for both humans and computers. Machine-readable metadata are essential for automatic discovery of datasets and services. In the context of ELNs, this means that all data and metadata stored in the notebook should be easily searchable and identifiable.\nAccessible: Once the data is found, it should be accessible. In ELNs, this means that the data should be readily available for retrieval and use, with clear and well-documented protocols for access[1].\nInteroperable: The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing. In the context of ELNs, this means that the data should be in formats that are compatible with other data sets and can be easily integrated into different workflows.\nReusable: The ultimate goal of FAIR is to optimize the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings. In the context of ELNs, this means that all data and metadata should be thoroughly documented, with clear information about the methods used to collect the data, the conditions under which the data was collected, and any other information that might be necessary for someone else to replicate the experiment or reuse the data[1].\nImplementing the FAIR principles in laboratory astrophysics using ELNs can significantly enhance the management and stewardship of research data, making it more findable, accessible, interoperable, and reusable for both humans and machines. This not only improves the efficiency and reproducibility of research but also promotes open science and data sharing among the scientific community."
  },
  {
    "objectID": "FAIR Astro.html#instrument",
    "href": "FAIR Astro.html#instrument",
    "title": "RDM 4 Astro",
    "section": "Instrument",
    "text": "Instrument\nIn the context of astrophysics instrument data management, the FAIR principles are closely connected to the International Virtual Observatory Alliance (IVOA) standards. Data management systems that have implemented these standards are close to FAIR compliance. However, additional components are required to make astronomical data fully FAIR. These may include the creation of rich metadata, the use of globally unique identifiers such as digital object identifiers (DOI), and the provision of data in machine-readable formats using standard vocabularies.\nA case study of the All-Sky Virtual Observatory, Australia’s node of the Virtual Observatory, provides an example of the implementation of the FAIR principles in astrophysics. This includes making data findable and accessible to both humans and machines, ensuring interoperability with other data, applications, and workflows, and enabling the reuse of data by providing well-described metadata and data.\nElectronic Lab Notebooks (ELNs) can also play a crucial role in astrophysics instrument data management. ELNs provide the infrastructure for data recording, retrieval, and integrity. They offer features such as time stamps to record data creation and modification, an audit trail that records an accessible version history of every change to an entry, and user-level restrictions that prevent unauthorized editing of entries. ELNs can also integrate with other software to directly read and write notebook entries, streamlining routine measurements and data capture\nThe FAIR principles and ELNs can significantly enhance astrophysics instrument data management by improving data findability, accessibility, interoperability, and reusability. However, their successful implementation requires careful planning, sufficient resources, and a clear understanding of the specific needs and workflows of the research team."
  },
  {
    "objectID": "BestPractiesRDM.html",
    "href": "BestPractiesRDM.html",
    "title": "RDM 4 Astro",
    "section": "",
    "text": "The module introduces the concept of data organization and good file management practices.\nData organization is important because as the research project progresses, a large volume of data is accumulated, and it can be difficult to find specific data files if they are named inaccurately or inconsistently.\nGood file management practices help identify, locate, and use data effectively, and file naming conventions are important when sharing data with collaborators.\nResearch data files and folders need to be labeled and organized in a systematic way to be identifiable and accessible for current and future users.\nConsistent data filing labeling has numerous benefits, including distinguishable data files, easier browsing and retrieval, logical sorting, and prevention of accidental deletion or overwrite.\nGood data file naming prevents confusion when multiple people are working on shared files.\n\n\n\n\n\nThree main criteria to consider when naming research data files:\n\norganization,\ncontext, and\nconsistency.\n\nCommon elements to consider when developing a file naming strategy include version number, date of creation, creator’s name, content description, team or department name, publication date, and project number.\nFile naming policy should be scalable and avoid generic names to prevent conflicts.\nFile names should be kept short, relevant, and consistent in format, and should not use special characters.\nBatch renaming software can be used to manage large numbers of files and automate consistent naming conventions.\nBulk renaming tools are available for different operating systems, and can be useful in situations such as assigning sequential numbers, using default names, or transferring files between systems with different naming conventions.\n\n\n\n\n\nDistinguish between different versions of data files consistently\nPick a clear versioning method (e.g. using ordinal numbers and decimals)\nAvoid confusing labels such as revision, final, final2, or definitive copy\nRecord changes to data files, even small ones, using auto backup or tracking facilities\nUse version control software such as Subversion and TortoiseSVN for software code\nDelete or discard obsolete versions of data files while retaining original copies\nFurther resources are available for additional information\nMove on to the data file formats module as the next step."
  },
  {
    "objectID": "BestPractiesRDM.html#organizing-data",
    "href": "BestPractiesRDM.html#organizing-data",
    "title": "RDM 4 Astro",
    "section": "",
    "text": "The module introduces the concept of data organization and good file management practices.\nData organization is important because as the research project progresses, a large volume of data is accumulated, and it can be difficult to find specific data files if they are named inaccurately or inconsistently.\nGood file management practices help identify, locate, and use data effectively, and file naming conventions are important when sharing data with collaborators.\nResearch data files and folders need to be labeled and organized in a systematic way to be identifiable and accessible for current and future users.\nConsistent data filing labeling has numerous benefits, including distinguishable data files, easier browsing and retrieval, logical sorting, and prevention of accidental deletion or overwrite.\nGood data file naming prevents confusion when multiple people are working on shared files.\n\n\n\n\n\nThree main criteria to consider when naming research data files:\n\norganization,\ncontext, and\nconsistency.\n\nCommon elements to consider when developing a file naming strategy include version number, date of creation, creator’s name, content description, team or department name, publication date, and project number.\nFile naming policy should be scalable and avoid generic names to prevent conflicts.\nFile names should be kept short, relevant, and consistent in format, and should not use special characters.\nBatch renaming software can be used to manage large numbers of files and automate consistent naming conventions.\nBulk renaming tools are available for different operating systems, and can be useful in situations such as assigning sequential numbers, using default names, or transferring files between systems with different naming conventions.\n\n\n\n\n\nDistinguish between different versions of data files consistently\nPick a clear versioning method (e.g. using ordinal numbers and decimals)\nAvoid confusing labels such as revision, final, final2, or definitive copy\nRecord changes to data files, even small ones, using auto backup or tracking facilities\nUse version control software such as Subversion and TortoiseSVN for software code\nDelete or discard obsolete versions of data files while retaining original copies\nFurther resources are available for additional information\nMove on to the data file formats module as the next step."
  },
  {
    "objectID": "BestPractiesRDM.html#file-formats-and-transformations",
    "href": "BestPractiesRDM.html#file-formats-and-transformations",
    "title": "RDM 4 Astro",
    "section": "File Formats and Transformations",
    "text": "File Formats and Transformations\n\nFile Formats\n\nThis module covers file formats, compression, data normalization, and transformations.\nFile formats encode information in a computer file, and software needs to recognize that format to access the content within it.\nThe file format is indicated by an extension in the file name, and files in proprietary formats may require specific software to open them, while open formats can be opened by multiple applications.\nFile types are based on text or binary encoding, and creating or saving data in a text format makes the file human-readable and able to be opened in any operating system.\nOpen, non-proprietary, and widely used file formats are less likely to become obsolete and more likely to be readable well into the future.\nConverting or migrating data files from one format to another may be necessary, and checksum algorithm tools can be used to compare file bits and ensure data integrity.\nCompression involves encoding information in fewer bits than the original representation and can result in lossy or lossless compression.\nZip is a de facto standard lossless compression format used on multiple platforms, while tar files are commonly used in Unix or Linux to bundle multiple files.\n\n\n\nData Transformations\n\nData transformations can be done for various reasons during or after a project\nData transformations involve changing the actual data\nAnonymization is an example of a data transformation that can be used in survey data\nQualitative data can be transformed into quantitative data using coding techniques\nData can be transformed to visualize it more effectively, such as converting ratios to percentages for display on charts\nConfidential or sensitive data can be transformed using aggregation or anonymization\nFurther reading resources are available for file formats, compression, normalization, and data transformations\nThe next module on documentation and data citation is recommended"
  },
  {
    "objectID": "BestPractiesRDM.html#documentation-and-data-citation",
    "href": "BestPractiesRDM.html#documentation-and-data-citation",
    "title": "RDM 4 Astro",
    "section": "Documentation and Data Citation",
    "text": "Documentation and Data Citation\n\nDocumentation\n\nThis module covers documentation and data citation.\nDocumentation is important for both the creator and other users to understand the data.\nExamples of data documentation include lab notebooks, codebooks, and methodology reports.\nThere are three levels of data documentation: project level, file/database level, and variable/item level.\nProper data citation is important for the credibility and accessibility of research findings.\nData citation should include information on how to access the underlying data.\n\n\n\nData Citation\n\nThe Joint Declaration on Data Citation Principles was issued in 2014 by Force11.\nCiting data helps identify and acknowledge it, promote reproducibility, track usage and impact, and recognize and reward data creators.\nDataCite recommends five minimum citation elements: creator, year of publication, title, publisher, and identifier.\nAdditional elements that may be added are Version and ResourceType.\nThe UK data service recommends using a title that indicates subject matter, geography, and time period.\nWhen citing data, adopt the same style and order of references as your other works, provide more information than less, and include a date of download for dynamic databases.\nGood practice in data documentation and citation contributes to reproducibility of research.\nEven if the data are unpublished, citation principles still apply.\nFurther reading and a next module on storage and security are available."
  },
  {
    "objectID": "BestPractiesRDM.html#storage-and-security",
    "href": "BestPractiesRDM.html#storage-and-security",
    "title": "RDM 4 Astro",
    "section": "Storage and Security",
    "text": "Storage and Security\n\nStorage\n\nThis module focuses on storing, securing, and backing up research data.\nLosing data can have serious consequences, and hard disk drive crashes are the most common cause of data loss.\nIt’s important to store and back up data securely from the outset, and to plan for storage needs and data management costs.\nNetwork drives are highly recommended as they provide a single copy of data that is backed up regularly and held securely.\nPCs, laptops, and external storage devices can also be used but should not be used as the master copy of data.\nCDs, DVDs, and magnetic tapes degrade over time, and errors writing to them are common, so high-quality products from good manufacturers should be used and periodically refreshed.\n\n\n\nBackup\n\nRegular backups are essential for data management to prevent loss due to hard drive failure or accidental deletion.\nThe 3-2-1 principle of backup involves having three copies of files on at least two different media, with one copy stored offsite.\nRegular testing of backups is crucial to ensure they can be restored if needed.\nSeveral questions should be considered when creating a backup strategy, such as how to back up data, how often to back up data, whether to use incremental or full backups, and how to keep track of different versions of data.\nVarious cloud services are available for data backup, including Dropbox, Google Drive, and OneDrive.\nAdvantages of cloud services for backup include no user intervention required, remote offsite backup, encryption and versioning, and multi-platform support.\nRisks of using cloud services for backup include data stored outside the European economic area, slow data restoration, unencrypted data, and service provider bankruptcy.\n\n\n\nData security\n\nData security means keeping your research data safe from damage, theft, breach of confidentiality, and premature release.\nConsider who needs access to the data and how to enforce permissions and restrictions.\nHave a clear policy on who can make copies of the data and store them on mobile devices.\nInstall up-to-date antivirus software and consider physical security for highly sensitive data.\nUse strong user names and passwords, avoid obvious phrases, don’t write them down, and don’t use the same password for multiple accounts.\nAvoid signing into secure sites from untrusted computers or networks.\n\n\n\nEncryption\n\nEncryption is the process of converting data into an unreadable code that requires an encryption key or password to be accessed.\nEncryption protects data from disclosure in case of loss or theft of a laptop or storage device.\nMedium or high-risk personal or business information must be encrypted if it leaves the university environment.\nA strong encryption password is necessary for data protection, and a reliable backup procedure is essential for password management.\nDifferent encryption software packages are available, and IT support can advise on encrypted flash drive purchases.\nFile deletion is not enough to remove sensitive data from a computer, and three main options for permanent data removal include data erasure, degaussing, and physical destruction.\nFurther reading and resources are available for storage, backup, and security."
  },
  {
    "objectID": "ELabNotebook.html",
    "href": "ELabNotebook.html",
    "title": "Electronic Lab Notebook (ELN)",
    "section": "",
    "text": "An Electronic Lab Notebook (ELN) is a digital tool that mimics the format of a paper lab notebook, allowing researchers to record protocols, observations, notes, and other data using their computer or mobile device. ELNs offer numerous benefits compared to traditional paper notebooks, such as promoting good data management practices, ensuring data security, facilitating auditing, and enabling collaboration. Certain ELNs can also manage inventories, track equipment maintenance schedules, and provide specialized scientific tools for tasks like chemical drawing or molecular biology.\n\n\nELNs align with FAIR Principles (Findable, Accessible, Interoperable, Reusable), endorsed by the research community and organizations like the National Institutes of Health. They enable effective oversight by Principal Investigators (PIs) or Core Facility managers. Collaborative data sharing and documentation become effortless with ELNs. They eliminate challenges arising from illegible handwriting and damaged paper notebooks. ELNs also safeguard against data loss when researchers transition. Certain ELNs integrate seamlessly with platforms like Mendeley, Dataverse, PubMed, and other applications, streamlining the publishing and research processes.\n\n\n\n1. Dedicated Tablets: Providing researchers with dedicated tablets specifically for lab use can help streamline computer-related tasks in the lab setting.\n2. Voice Input or OCR Plugins: Utilizing voice input or optical character recognition (OCR) plugins can facilitate data entry, making it easier and faster to record observations and other experiment-related information.\n3. Time-Saving Features: Leveraging time-saving features within an Electronic Lab Notebook (ELN), such as linking experiments to raw data files and results, and automatically timestamping entries, helps maintain data provenance and efficiency.\n4. Integration with Research Software: Integrating the ELN with other research software enables seamless data capture and information management, simplifying the overall experimental process.\n\n\n\n\n\nLabguru is a comprehensive, cloud-based platform that combines features of an Electronic Lab Notebook (ELN), Laboratory Information Management System (LIMS), and informatics software. Designed for life science research and industry, Labguru offers a secure solution for managing laboratory data, inventory, and workflows.\nThis platform includes molecular biology and chemistry tools, facilitating experimental design, data capture, and project management. With Labguru, scientists can create customizable experiment templates, integrate protocols and standard operating procedures (SOPs), and collaborate on research projects. The platform enhances data quality, streamlines workflows, and reduces costs.\nLabguru is accessible on both desktop and mobile devices through cloud-based access. It is a part of the Holtzbrinck Publishing Group and serves a diverse user base of over 120,000 scientists worldwide. Its users range from startups, universities, and research institutes to some of the largest pharmaceutical companies.\n\n\n\nSciNote is an advanced cloud-based electronic lab notebook (ELN) that is trusted by prestigious organizations like the FDA, USDA, NIH, and the European Commission. It has a user base of over 90,000 scientists across 100 countries. SciNote stands out for its comprehensive features, including inventory management, compliance tools, and team management capabilities. It offers top-rated data management functionalities such as inventory tracking, protocol and SOP management, regulatory compliance, team collaboration, integrations, project management, and robust data security measures. With headquarters in Middleton, WI, USA, SciNote also has offices in Europe to serve its global community of users.\n\n\n\nLabWare is a renowned global leader in laboratory information management systems (LIMS). With over 30 years of experience, LabWare has served more than 3,000 customers worldwide, including prestigious organizations such as NIH, USDA, GSK, Pfizer, Hershey, Caterpillar, and Chevron. LabWare boasts an impressive 98% customer satisfaction rate.\nLabWare’s laboratory automation platform is designed to ensure data integrity, compliance, and accurate test results, ultimately leading to more efficient laboratory operations. Customers have the flexibility to choose between two options: the cost-optimized and fully validated LabWare SaaS LIMS or the industry-optimized and fully customizable LabWare LIMS/ELN. Both solutions cater to the specific needs of laboratories, offering advanced functionality and reliable performance."
  },
  {
    "objectID": "ELabNotebook.html#why-should-we-use-eln",
    "href": "ELabNotebook.html#why-should-we-use-eln",
    "title": "Electronic Lab Notebook (ELN)",
    "section": "",
    "text": "ELNs align with FAIR Principles (Findable, Accessible, Interoperable, Reusable), endorsed by the research community and organizations like the National Institutes of Health. They enable effective oversight by Principal Investigators (PIs) or Core Facility managers. Collaborative data sharing and documentation become effortless with ELNs. They eliminate challenges arising from illegible handwriting and damaged paper notebooks. ELNs also safeguard against data loss when researchers transition. Certain ELNs integrate seamlessly with platforms like Mendeley, Dataverse, PubMed, and other applications, streamlining the publishing and research processes."
  },
  {
    "objectID": "ELabNotebook.html#using-computers-during-experiments",
    "href": "ELabNotebook.html#using-computers-during-experiments",
    "title": "Electronic Lab Notebook (ELN)",
    "section": "",
    "text": "1. Dedicated Tablets: Providing researchers with dedicated tablets specifically for lab use can help streamline computer-related tasks in the lab setting.\n2. Voice Input or OCR Plugins: Utilizing voice input or optical character recognition (OCR) plugins can facilitate data entry, making it easier and faster to record observations and other experiment-related information.\n3. Time-Saving Features: Leveraging time-saving features within an Electronic Lab Notebook (ELN), such as linking experiments to raw data files and results, and automatically timestamping entries, helps maintain data provenance and efficiency.\n4. Integration with Research Software: Integrating the ELN with other research software enables seamless data capture and information management, simplifying the overall experimental process."
  },
  {
    "objectID": "ELabNotebook.html#eln-software",
    "href": "ELabNotebook.html#eln-software",
    "title": "Electronic Lab Notebook (ELN)",
    "section": "",
    "text": "Labguru is a comprehensive, cloud-based platform that combines features of an Electronic Lab Notebook (ELN), Laboratory Information Management System (LIMS), and informatics software. Designed for life science research and industry, Labguru offers a secure solution for managing laboratory data, inventory, and workflows.\nThis platform includes molecular biology and chemistry tools, facilitating experimental design, data capture, and project management. With Labguru, scientists can create customizable experiment templates, integrate protocols and standard operating procedures (SOPs), and collaborate on research projects. The platform enhances data quality, streamlines workflows, and reduces costs.\nLabguru is accessible on both desktop and mobile devices through cloud-based access. It is a part of the Holtzbrinck Publishing Group and serves a diverse user base of over 120,000 scientists worldwide. Its users range from startups, universities, and research institutes to some of the largest pharmaceutical companies.\n\n\n\nSciNote is an advanced cloud-based electronic lab notebook (ELN) that is trusted by prestigious organizations like the FDA, USDA, NIH, and the European Commission. It has a user base of over 90,000 scientists across 100 countries. SciNote stands out for its comprehensive features, including inventory management, compliance tools, and team management capabilities. It offers top-rated data management functionalities such as inventory tracking, protocol and SOP management, regulatory compliance, team collaboration, integrations, project management, and robust data security measures. With headquarters in Middleton, WI, USA, SciNote also has offices in Europe to serve its global community of users.\n\n\n\nLabWare is a renowned global leader in laboratory information management systems (LIMS). With over 30 years of experience, LabWare has served more than 3,000 customers worldwide, including prestigious organizations such as NIH, USDA, GSK, Pfizer, Hershey, Caterpillar, and Chevron. LabWare boasts an impressive 98% customer satisfaction rate.\nLabWare’s laboratory automation platform is designed to ensure data integrity, compliance, and accurate test results, ultimately leading to more efficient laboratory operations. Customers have the flexibility to choose between two options: the cost-optimized and fully validated LabWare SaaS LIMS or the industry-optimized and fully customizable LabWare LIMS/ELN. Both solutions cater to the specific needs of laboratories, offering advanced functionality and reliable performance."
  },
  {
    "objectID": "Archiving.html",
    "href": "Archiving.html",
    "title": "RDM 4 Astro",
    "section": "",
    "text": "Archiving digital data is crucial for long-term accessibility and usability.\nTrustworthy repositories can provide strategies to preserve data authenticity and integrity.\nUnderstanding preservation needs, authenticity, integrity, and metadata is important.\nDigital data are at risk due to benign neglect, bit rot, obsolescence, and insufficient documentation.\nBit rot is the degradation or corruption of machine-readable information over time.\nObsolescence occurs when hardware or software becomes outdated and data becomes trapped.\nInsufficient documentation makes it impossible to interpret data in the future.\nComplete and appropriate documentation and metadata are essential for long-term data preservation.\n\n\n\n\n\nPreservation of digital content aims to ensure authenticity and integrity\nAuthenticity means the data is genuine and free from tampering\nEstablishing authenticity requires both researchers and data repositories to have procedures and documentation in place\nBest practices for maintaining authenticity include maintaining a single master file, regulating write access, recording all changes, and archiving copies of master files\nIntegrity means the digital object has not been corrupted over time or in transit between storage locations or systems\nBest practices for maintaining integrity include backing up critical files, storing master files in open source formats, verifying backup copies, storing copies on two types of storage media, and copying/migrating files to new storage media every 2-5 years\nTechnology has increased our ability to collect and analyze data but also has vulnerabilities, requiring an active preservation strategy based on standards and best practices.\n\n\n\n\n\nMetadata is structured information that describes, explains, locates, or represents something else and is necessary for resource discovery, organization, interoperability, identification, and preservation.\nThere are three types of metadata: descriptive, administrative, and structural.\nDescriptive metadata is used for discovery and identification, including information such as title, author, and abstract.\nAdministrative metadata is structured information regarding the management and tracking of data over time and can be rights management or preservation metadata.\nStructural metadata describes the physical or logical structure of digital objects.\nMetadata standards often require adherence to specific representation rules and controlled vocabularies to ensure consistency in data entry and authoritative use of terms.\nGood standardized metadata allows for interoperability across systems, data structures, and interfaces, and facilitates the efficient dissemination of digital materials.\nThe open archive initiative protocol for metadata harvesting (OAI-PMH) is an example of how standardized metadata can be shared across distributed systems."
  },
  {
    "objectID": "Archiving.html#preservation-foundations",
    "href": "Archiving.html#preservation-foundations",
    "title": "RDM 4 Astro",
    "section": "",
    "text": "Archiving digital data is crucial for long-term accessibility and usability.\nTrustworthy repositories can provide strategies to preserve data authenticity and integrity.\nUnderstanding preservation needs, authenticity, integrity, and metadata is important.\nDigital data are at risk due to benign neglect, bit rot, obsolescence, and insufficient documentation.\nBit rot is the degradation or corruption of machine-readable information over time.\nObsolescence occurs when hardware or software becomes outdated and data becomes trapped.\nInsufficient documentation makes it impossible to interpret data in the future.\nComplete and appropriate documentation and metadata are essential for long-term data preservation.\n\n\n\n\n\nPreservation of digital content aims to ensure authenticity and integrity\nAuthenticity means the data is genuine and free from tampering\nEstablishing authenticity requires both researchers and data repositories to have procedures and documentation in place\nBest practices for maintaining authenticity include maintaining a single master file, regulating write access, recording all changes, and archiving copies of master files\nIntegrity means the digital object has not been corrupted over time or in transit between storage locations or systems\nBest practices for maintaining integrity include backing up critical files, storing master files in open source formats, verifying backup copies, storing copies on two types of storage media, and copying/migrating files to new storage media every 2-5 years\nTechnology has increased our ability to collect and analyze data but also has vulnerabilities, requiring an active preservation strategy based on standards and best practices.\n\n\n\n\n\nMetadata is structured information that describes, explains, locates, or represents something else and is necessary for resource discovery, organization, interoperability, identification, and preservation.\nThere are three types of metadata: descriptive, administrative, and structural.\nDescriptive metadata is used for discovery and identification, including information such as title, author, and abstract.\nAdministrative metadata is structured information regarding the management and tracking of data over time and can be rights management or preservation metadata.\nStructural metadata describes the physical or logical structure of digital objects.\nMetadata standards often require adherence to specific representation rules and controlled vocabularies to ensure consistency in data entry and authoritative use of terms.\nGood standardized metadata allows for interoperability across systems, data structures, and interfaces, and facilitates the efficient dissemination of digital materials.\nThe open archive initiative protocol for metadata harvesting (OAI-PMH) is an example of how standardized metadata can be shared across distributed systems."
  },
  {
    "objectID": "Archiving.html#trustworthy-repositories",
    "href": "Archiving.html#trustworthy-repositories",
    "title": "RDM 4 Astro",
    "section": "Trustworthy Repositories",
    "text": "Trustworthy Repositories\n\nDemonstrating Trustworthiness\n\nTrustworthy repositories are crucial for the long-term preservation of data.\nData repositories oversee the long-term storage and preservation of data and come in different types, including domain, institutional, and location-specific.\nIt’s important to talk to repository staff early in the research data life cycle to understand specific data management requirements.\nTo ensure the trustworthiness of a repository, they must demonstrate compliance with standards and best practices through transparent policies and seek audit and certification.\nAdhering to standards and best practices is essential to achieving trustworthiness.\nTransparent policies and procedures are necessary for repositories to demonstrate their trustworthiness, and they should make their archival and digital preservation policies readily available to stakeholders.\nThe Data Seal of Approval, DRAMBORA, and ISO 16363 Audit and Certification of Trustworthy Digital Repositories are examples of audit schemes used to assess the trustworthiness of digital repositories based on adherence, standards, and best practices.\n\n\n\nData Curation Standards and Best Practices (Part 1)\n\nISO 14721 is a reference model for an open archival information system (OAIS).\nOAIS defines elements and processes within digital repositories, and establishes responsibilities for long-term preservation of digital information.\nRepositories that consider themselves to be OAIS archives are presumed to have mechanisms and workflows in place to properly safeguard digital materials for a designated community.\nOAIS model has six functional entities: Ingest, Archival Storage, Data Management, Administration, Preservation, and Access.\nIngest function performs several tasks to establish evidence of authenticity, ensure files are in proper formats, and normalize files to formats optimal for long-term preservation.\nStandard ingest procedures include consideration of sustainability and use factors that affect feasibility and cost of long-term file preservation.\nPreservation optimization presumes that supplementary documentation is included with the file to enable appropriate interpretation and use of the data.\nOnce ingest tasks have been completed, the SIP becomes an AIP which is managed by the archival storage function.\nData management function provides functionality for generating, maintaining, and accessing metadata to document files housed in archival storage.\nAccess function coordinates requests and delivers the final dissemination information package (DIP) to users who can retrieve data files.\n\n\n\nData Curation Standards and Best Practices (Part 2)\n\nPreservation planning function provides recommendations for preservation and planning strategies to ensure that data are accessible and understandable to users over time.\nMigration and emulation are common preservation strategies to protect against data loss due to obsolescence.\nPreservation planning involves ongoing evaluation of archival materials to identify file migration requirements, recommend changes to archive processes and policies, and report any risks.\nAdministration function handles data submission agreements with data producers, establishes quality standards, and manages infrastructure configurations.\nTrustworthy repositories have short-term and long-term business plans for sustainability with transparent accounting practices, and they analyze and document financial risks, investments and expenditures.\nStoring copies of digital content in geographically distributed locations and collaborative partnerships can help protect against physical losses from disasters or organizational failure.\nEconomics sustainability for long term preservation is an ongoing concern in the field of data archiving because the cost of long term preservation is still largely unclear.\nTrustworthy data repositories make a verifiable commitment to archiving data and expend significant amounts of labor, funding, research and assessment to safeguard data."
  },
  {
    "objectID": "DataManagement.html",
    "href": "DataManagement.html",
    "title": "RDM 4 Astro",
    "section": "",
    "text": "Data management supports long-term preservation, access, and use of data\nActivities of data management include planning, documenting, formatting, storing, anonymizing, and controlling access to data\nManaging data helps researchers optimize the use of data, collaborate with other researchers, and answer additional questions in the future\nData management is important for responsible research conduct and meeting requirements from funding agencies, journal publishers, and research institutions\nData management sustains the value of data and increases transparency in research projects"
  },
  {
    "objectID": "DataManagement.html#understanding-data-management",
    "href": "DataManagement.html#understanding-data-management",
    "title": "RDM 4 Astro",
    "section": "",
    "text": "Data management supports long-term preservation, access, and use of data\nActivities of data management include planning, documenting, formatting, storing, anonymizing, and controlling access to data\nManaging data helps researchers optimize the use of data, collaborate with other researchers, and answer additional questions in the future\nData management is important for responsible research conduct and meeting requirements from funding agencies, journal publishers, and research institutions\nData management sustains the value of data and increases transparency in research projects"
  },
  {
    "objectID": "DataManagement.html#what-are-data",
    "href": "DataManagement.html#what-are-data",
    "title": "RDM 4 Astro",
    "section": "What Are Data?",
    "text": "What Are Data?\n\nResearch Data Defined\n\nThe fundamental question of the course is what are data, and different organizations have tackled it resulting in various definitions.\nData are different for various disciplines and contexts, and there are multiple types of data in an array of contexts, including numeric and textual data, biological samples, and physical collections.\nResearch data and associated researcher materials should be distinguished, and some of them may be required alongside the data to understand them.\nThe most important concept in the entire course is understanding data in the context of the research data life cycle, from project planning to archiving that data as a research output after the project period has ended.\nKey concepts in these definitions include validity, data sharing among the community, heterogeneity, and contextualization within research communities.\n\n\n\nTypes of Data and Metadata\n\nTypes of data include numeric/tabular data, samples (e.g. DNA, blood), physical collections (e.g. plant specimens), software programs/code, databases, algorithms, models, and geodatabases.\nBackground data provides contextual information for analysis and includes questionnaires, code books, and descriptions of methodologies.\nResearch products built on data include reports, conference posters, articles, white papers, books, websites, and blogs.\nMetadata is structured information that describes, explains, locates, or represents something else (in this case, research data).\nMinimum metadata elements include who created the data, when it was created/published, and a descriptive name for the dataset. A unique identifier is also necessary to locate the data."
  },
  {
    "objectID": "DataManagement.html#data-management-planning",
    "href": "DataManagement.html#data-management-planning",
    "title": "RDM 4 Astro",
    "section": "Data Management Planning",
    "text": "Data Management Planning\n\nIntroduction to Data Management Plans\n\nData management is important in the research life cycle.\nA data management plan (DMP) is a formal document that outlines data management strategies during and after a research project.\nMany funding agencies require grantees to submit a DMP as part of the proposal package.\nWriting a comprehensive DMP can encourage researchers to think carefully about data management needs.\nThe transparency and openness of publicly funded research data requires it to be discoverable, accessible, and reusable to the public.\nA well-managed data plan can provide a greater return on investment and benefits for verification, reduction of scientific fraud, promotion of new research, resources for training new researchers, and discouraging unintentional redundancy in research.\n\n\n\nData Management Plan Content\n\nA data management plan (DMP) should describe all major aspects of data management throughout the research life cycle.\nThe Digital Curation Center provides a checklist for a data management plan with questions that should be addressed in the DMP.\nThe checklist includes questions such as: what data will you collect or create? How will the data be collected or created? What metadata documentation will accompany the data? How will you manage ethical and legal issues related to data ownership, privacy, and copyright? How will you store and backup the data? How will you manage access and security? Which data should be retained, shared, and/or preserved? What is the long-term preservation plan for the dataset? How will you share the data? Who will be responsible for data management? What resources will you require to implement your plan?\nThe DMP should justify the data format choice and include storage implications due to the format or volume of the data.\nThe DMP should describe the data collection methods, organizing data files, applying version control, and implementing quality assurance protocols.\nThe DMP should identify provisions for protecting the confidentiality of human participants if the project involves research with human subjects.\nThe DMP should identify the owner of the data and discuss permissions with the data producer if third-party data is being reused.\nThe DMP should include provisions for systematic backups of the data files and describe security measures in detail.\nThe DMP should identify the repository to be used to archive the data and plans to prepare and document the data for long-term preservation.\nThe DMP should identify the mechanism for sharing the data and any restrictions required for data sharing.\nThe DMP should identify who will oversee the implementation of the data management plan and what resources will be required for data management tasks and long-term preservation."
  },
  {
    "objectID": "survey.html",
    "href": "survey.html",
    "title": "RDM 4 Astro",
    "section": "",
    "text": "The FAIR principles, which stand for Findable, Accessible, Interoperable, and Reusable, are a set of guidelines proposed by a consortium of scientists and organizations to support the reusability of digital assets[1]. These principles are designed to ensure that data is easily located, retrieved, and used, and that the necessary metadata is provided to allow others to understand and reuse the data[2]. Here are questions that could be included in a survey for institutions working on astronomical data to assess their adherence to the FAIR principles:"
  },
  {
    "objectID": "survey.html#findable",
    "href": "survey.html#findable",
    "title": "RDM 4 Astro",
    "section": "Findable",
    "text": "Findable\n\nDo you assign a unique and persistent identifier to each dataset?\nAre your datasets easily discoverable through search engines or data discovery tools?\nDo you provide detailed metadata for each dataset?\nIs the metadata consistently structured and standardized?\nDo you use community-accepted vocabularies in your metadata?\nAre your datasets indexed in a searchable resource?\nDo you have a system in place to ensure the persistence of dataset identifiers?\nDo you use standard identifier schemes (e.g., DOI, ORCID) for your datasets?\nAre your datasets and their metadata findable by machines (i.e., machine-readable)?\nDo you provide clear instructions on how to find your datasets?"
  },
  {
    "objectID": "survey.html#accessible",
    "href": "survey.html#accessible",
    "title": "RDM 4 Astro",
    "section": "Accessible",
    "text": "Accessible\n\nAre your datasets openly accessible online?\nIf your datasets are not openly accessible, do you provide clear and accessible information about how to access them?\nDo you use standard protocols for data access?\nAre your datasets accessible even when the data are no longer available?\nAre your datasets accessible in a way that is compliant with relevant data protection and privacy regulations?\nDo you provide an API for programmatically accessing your datasets?\nAre your datasets accessible without proprietary software?\nDo you provide support for users who have difficulty accessing your datasets?\nAre your datasets and their metadata accessible by machines (i.e., machine-readable)?\nDo you provide clear instructions on how to access your datasets?"
  },
  {
    "objectID": "survey.html#interoperable",
    "href": "survey.html#interoperable",
    "title": "RDM 4 Astro",
    "section": "Interoperable",
    "text": "Interoperable\n\nDo you use community-accepted vocabularies in your datasets?\nAre your datasets compatible with other datasets in your field?\nDo you provide metadata that allows for the integration of your datasets with other datasets?\nDo you use standard data formats for your datasets?\nAre your datasets and their metadata interoperable with other systems and platforms?\nDo you use standard data models for your datasets?\nDo you provide tools or services to facilitate data interoperability?\nAre your datasets interoperable with common data analysis tools?\nAre your datasets and their metadata interoperable by machines (i.e., machine-readable)?\nDo you provide clear instructions on how to integrate your datasets with other datasets?"
  },
  {
    "objectID": "survey.html#reusable",
    "href": "survey.html#reusable",
    "title": "RDM 4 Astro",
    "section": "Reusable",
    "text": "Reusable\n\nDo you provide a clear and accessible data usage license for each dataset?\nDo you provide detailed provenance information for each dataset?\nAre your datasets accompanied by comprehensive documentation?\nDo you provide information about the data collection methods used for each dataset?\nDo you provide information about the quality control procedures applied to each dataset?\nAre your datasets reusable in different contexts?\nDo you provide tools or services to facilitate data reuse?\nAre your datasets reusable with common data analysis tools?\nAre your datasets and their metadata reusable by machines (i.e., machine-readable)?\nDo you provide clear instructions on how to reuse your datasets?"
  },
  {
    "objectID": "survey.html#general-questions",
    "href": "survey.html#general-questions",
    "title": "RDM 4 Astro",
    "section": "General Questions",
    "text": "General Questions\n\nDo you have a data management plan that incorporates the FAIR principles?\nDo you provide training or resources on the FAIR principles to your staff?\nHave you encountered any challenges in implementing the FAIR principles?\nHow do you measure the success of your implementation of the FAIR principles?\nHave you seen any benefits from implementing the FAIR principles?\nDo you have any plans to improve your adherence to the FAIR principles?\nDo you collaborate with other institutions to promote the FAIR principles?\nDo you consider the FAIR principles when evaluating new data management tools or services?\nHow do you handle sensitive or personally identifiable information in accordance with the FAIR principles?\nHow do you ensure the long-term preservation of your FAIR data?"
  },
  {
    "objectID": "survey.html#fair-principles-and-software",
    "href": "survey.html#fair-principles-and-software",
    "title": "RDM 4 Astro",
    "section": "FAIR Principles and Software",
    "text": "FAIR Principles and Software\n\nDo you develop software as part of your research?\nDo you follow the FAIR principles when managing your software?\nHow do you ensure the findability of your software?\nHow do you ensure the accessibility of your software?\nHow do you ensure the interoperability of your software?\nHow do you ensure the reusability of your software?"
  },
  {
    "objectID": "survey.html#fair-principles-and-metadata",
    "href": "survey.html#fair-principles-and-metadata",
    "title": "RDM 4 Astro",
    "section": "FAIR Principles and Metadata",
    "text": "FAIR Principles and Metadata\n\nDo you follow the FAIR principles when managing your metadata?\nHow do you ensure the findability of your metadata?\nHow do you ensure the accessibility of your metadata?\nHow do you ensure the interoperability of your metadata?\nHow do you ensure the reusability of your metadata?"
  },
  {
    "objectID": "survey.html#fair-principles-and-data-sharing",
    "href": "survey.html#fair-principles-and-data-sharing",
    "title": "RDM 4 Astro",
    "section": "FAIR Principles and Data Sharing",
    "text": "FAIR Principles and Data Sharing\n\nDo you share your data with other researchers or institutions?\nDo you follow the FAIR principles when sharing your data?\nHow do you ensure the findability of your shared data?\nHow do you ensure the accessibility of your shared data?\nHow do you ensure the interoperability of your shared data?\nHow do you ensure the reusability of your shared data?"
  },
  {
    "objectID": "survey.html#fair-principles-and-data-publication",
    "href": "survey.html#fair-principles-and-data-publication",
    "title": "RDM 4 Astro",
    "section": "FAIR Principles and Data Publication",
    "text": "FAIR Principles and Data Publication\n\nDo you publish your data in data journals or repositories?\nDo you follow the FAIR principles when publishing your data?\nHow do you ensure the findability of your published data?\nHow do you ensure the accessibility of your published data?\nHow do you ensure the interoperability of your published data?\nHow do you ensure the reusability of your published data?\nDo you have any suggestions for improving the FAIRness of astronomical data?"
  },
  {
    "objectID": "survey.html#references",
    "href": "survey.html#references",
    "title": "RDM 4 Astro",
    "section": "References:",
    "text": "References:\n[1] https://library.cumc.columbia.edu/insight/what-are-fair-data-principles\n[2] https://www.nature.com/articles/sdata201618"
  },
  {
    "objectID": "Networking.html",
    "href": "Networking.html",
    "title": "Networking and Consortium",
    "section": "",
    "text": "PUNCH4NFDI is a consortium within the National Research Data Infrastructure (NFDI) of Germany. It represents the fields of particle, astro-, astroparticle, hadron, and nuclear physics, encompassing about 9,000 scientists from universities, the Max Planck Society, the Leibniz Association, and the Helmholtz Association in Germany[1][5].\nThe consortium’s primary goal is to establish a federated and “FAIR” science data platform, offering the infrastructures and interfaces necessary for the access to and use of data and computing resources of the involved communities and beyond. “FAIR” stands for Findable, Accessible, Interoperable, and Reusable, indicating that the corresponding data sets should be easy to find, easily accessible, linkable, and reusable[1][4].\nPUNCH4NFDI is organized into several task areas, which are key to effective collaboration within the consortium and with other consortia within the NFDI. These task areas include the grant applicant DESY, FIAS, and 18 other funding recipients. It also includes 22 partners from the Helmholtz Association, the Max Planck Society, the Leibniz Association, and universities, such as Goethe University[4].\nThe consortium’s work focuses on addressing the fundamental constituents of matter and their interactions, as well as their role in the development of the largest structures in the universe - stars and galaxies. The achievements of PUNCH science range from the discovery of the Higgs boson, the installation of a 1 cubic kilometre particle detector for neutrino detection in the Antarctic ice, the detection of the quark-gluon plasma in heavy-ion collisions, to the first picture ever of the black hole at the heart of the Milky Way[1][5].\nPUNCH4NFDI also offers services for the efficient scientific exploitation of research data. In doing so, it collaborates closely with its European and international partners at the forefront of research in scientific data management[1].\nThe consortium organizes various events and workshops, such as the “ErUM-Data-Hub: Workshop on Inverse Problems” and “Radio 2023”. These events are aimed at fostering collaboration and knowledge exchange within the scientific community[2].\nIn terms of governance, an Executive Board with representatives from the PUNCH community oversees PUNCH4NFDI. A Management Board, including task area and work package leaders, coordinates and monitors progress and resource allocation/usage. An external Science Advisory Committee of physicists and computer scientists provides advice on ongoing and future developments. An elected User Committee ensures feedback and input from the user side regarding the scientific directions of the project and the services offered. An Infrastructure Control Board is responsible for synchronizing the top-level requirements and deliverables with the national and international data providers[7].\n\n\n\nThe National Research Data Infrastructure (NFDI) is a German initiative designed to systematically manage scientific and research data, provide long-term data storage, backup and accessibility, and network the data both nationally and internationally. The NFDI is developed step by step and science-oriented with services available to researchers from various disciplines, institutions, and federal states.\nWithin the NFDI, there are specific sections where association members work together to develop cross-consortium standards, metadata standards, and formats. These sections are legally dependent departments of the NFDI Association, and they work on cross-sectional topics across the boundaries of the consortia. The establishment of these sections is a strategy-led process initiated by the NFDI Directorate and the NFDI Consortium Assembly.\nFive sections have been established:\n\nCommon Infrastructures (section-infra)\nEthical, Legal and Social Aspects (section-ELSA)\n(Meta)data, Terminologies, Provenance (section-metadata)\nTraining & Education (section-edutrain)\nIndustry Engagement (section-industry) [1]\n\nEach section focuses on different aspects of research data management. For instance, the (Meta)data, Terminologies, Provenance section supports processes of the NFDI consortia in jointly developing and establishing common data and metadata standards as a foundation for effective reuse of research data in accordance with FAIR principles. This section also addresses issues of (meta)data harmonization, discoverability of data, general data and metadata standards, format conversions, and persistent identifier systems.\nThe Training & Education section, on the other hand, provides an overview of various training materials and further training measures/events in research data management in all disciplines in German-speaking countries.\nThe NFDI is governed by a Board of Trustees, a Scientific Senate, and a Directorate. The Board of Trustees is the administrative-strategic supervisory body of the NFDI Association. The Scientific Senate is the central strategic body of the NFDI, advising on all fundamental matters relating to the ongoing development of the national research data infrastructure. The Directorate, located in Karlsruhe, consists of a full-time director and a managing office. The Directorate coordinates the work and activities of NFDI consortia on cross-disciplinary topics.\n\n\n\n\nErUM-Data-Hub, funded by the Federal Ministry of Education and Research (BMBF), is a central networking and transfer office for the digital transformation in the exploration of universe and matter (ErUM). The main aims of the ErUM-Data-Hub are to network ErUM communities, identify and exploit synergies in ErUM-Data-projects in the field of digitalization, communicate research results to target groups, and facilitate knowledge and innovation transfer. Additionally, the ErUM-Data-Hub provides a diversified further education offer in the field of digital competence[1].\nErUM, which stands for “Erforschung von Universum und Materie,” involves about 20,000 scientists in Germany who are engaged in the exploration of the universe and matter using large-scale equipment. German ErUM research is organized in 8 ErUM committees, which coordinate and represent the interests of the different research areas[1].\nAs part of the ErUM-Data Plan of Action, the ErUM Data Hub pursues the overarching goal of digital transformation in fundamental ErUM-Pro research. In close coordination with the ErUM communities, the Hub implements the strategic measures of the ErUM Data Action Plan[1].\nThe ErUM-Data-Hub also organizes various events and workshops, such as the “Deep Learning School ‘Basic Concepts’” aimed at deep-learning starters from all ErUM communities who have a basic knowledge of physics[4].\nIn terms of research, progress in fundamental research on universe and matter (ErUM) is made by studying structures at smaller and smaller scales. The high resolution of modern instruments in particle, hadron and nuclear, and astroparticle physics results in huge amounts of research data. To address the challenge of increasing data rates and volumes, 17 partners from universities and research centers joined forces in the BMBF funded ErUM Data pilot project Innovative Digital Technologies for Research on Universe and Matter[2].\n\n\n\n\n\nHelmholtz DMA, or Data Management and Analysis, is a research initiative within the Helmholtz-Zentrum Dresden-Rossendorf (HZDR). The primary objective of Helmholtz DMA is the implementation of a comprehensive digitization strategy for the Research Field MATTER. This involves the development, application, provision, and integration of innovative digital solutions for handling and analyzing the extreme volumes and rates of complex data coming from machines, experiments, and simulations. The initiative also focuses on the research and application of artificial intelligence for knowledge extraction from experiments and simulations, and the application of frontier technologies such as exascale computing and quantum computing to model complex states of matter[1][4].\nHelmholtz DMA is strongly aligned with the overall activities of the topic, with strong integration with the activities in the topic “Accelerator Research and Development” and the topic “Matter – Dynamics, Mechanisms and Control” in the program MML[1]. The DMA initiative also creates synergies in Matter between Helmholtz Centers & Facilities, Research Programs (MU,MML,MT), and Science Domains. It develops open, shareable solutions for Matter, including open source, modular solutions, open standards, professional, industry-grade software development, and F.A.I.R. Data[4].\nIn addition to its research activities, Helmholtz DMA also organizes various events and workshops, such as the “Research Data Management - Experts Workshop. Synergies between NFDI (including DAPHNE4NFDI), ErUM-Data and Helmholtz-DMA” that took place in Bonn in March 2023[3].\n\n\n\nThe Helmholtz Metadata Collaboration (HMC) is an initiative within the Helmholtz Association that aims to promote the qualitative enrichment of research data through metadata. HMC develops and implements novel concepts and technologies for sustainable handling of research data through high-quality metadata. Its main goal is to make the depth and breadth of research data produced by Helmholtz Centres findable, accessible, interoperable, and reusable (FAIR) for the whole science community[1].\nHMC is organized into several hubs, each focusing on a specific research field. These hubs pool scientific expertise on metadata and provide advice and infrastructure services for storing, reusing, and exchanging metadata. The work of HMC is always embedded in the national and international context, and it aims to keep research data “alive” by making it accessible and usable to interested parties in various disciplines[6].\n\n\n\n\n\n\nThe B3D project is an interdisciplinary collaboration between universities and research institutions in North Rhine-Westphalia (NRW), Germany. It aims to create a prosperous network of experts in astrophysics and data science. The project uses powerful analysis and visualization techniques, sustainable technology, and infrastructure to develop solutions for the upcoming challenges of the next generation of astronomical observatories. This is done in close collaboration with other leading academic and industrial partners[2].\nOne of the main goals of the B3D project is to generate synergies between different areas of research and to provide a joint research-oriented training program in future technologies that can be applied to astronomical research and beyond[2]. The project also aims to develop new measures for the qualification of young scientists[3].\nA central component of the B3D project is the development of innovative traversers using artificial intelligence and machine learning. These tools are used to systematically manage and analyze the immense flood of data to ensure better understanding[3].\nIn terms of Research Data Management (RDM), the B3D project is a prime example of how RDM principles can be applied to handle and analyze large volumes of data in a scientific context. Effective RDM is a key component of research integrity and reproducible research, and its importance is increasingly recognized in data-intensive fields like radio astronomy[6].\n\n\n\nSFB 1601 is a collaborative research center that focuses on the study of the interstellar medium, star formation, and the evolution of galaxies. The project Inf - SFB 1601: RDM services and research product exchange is a part of this collaborative research center and aims to establish an interconnected infrastructure that ensures the documentation, preservation, and quality assessment of data and procedures. This infrastructure allows project partners to select and use the RDM services most suitable for their own needs, exchange research products (data, procedures, code, etc.) efficiently and transparently, and make all relevant scientific data accessible within the CRC and to the astronomical community as a whole.\nThe CRC research is based on four pillars: instrumentation, observations, 3D simulations, and laboratory astrophysics. This implies data and workflows of different types and specifications, which require different research data management (RDM) and research software management (RSM) solutions. The project aims to cover the gaps in RDM within the four pillars that were identified in the needs assessment and classified into three categories:\n\nDocumentation & legal aspects,\nRDM/RSM support along the data-life cycle,\nSkills development & Cooperation."
  },
  {
    "objectID": "Networking.html#nfdi",
    "href": "Networking.html#nfdi",
    "title": "Networking and Consortium",
    "section": "",
    "text": "PUNCH4NFDI is a consortium within the National Research Data Infrastructure (NFDI) of Germany. It represents the fields of particle, astro-, astroparticle, hadron, and nuclear physics, encompassing about 9,000 scientists from universities, the Max Planck Society, the Leibniz Association, and the Helmholtz Association in Germany[1][5].\nThe consortium’s primary goal is to establish a federated and “FAIR” science data platform, offering the infrastructures and interfaces necessary for the access to and use of data and computing resources of the involved communities and beyond. “FAIR” stands for Findable, Accessible, Interoperable, and Reusable, indicating that the corresponding data sets should be easy to find, easily accessible, linkable, and reusable[1][4].\nPUNCH4NFDI is organized into several task areas, which are key to effective collaboration within the consortium and with other consortia within the NFDI. These task areas include the grant applicant DESY, FIAS, and 18 other funding recipients. It also includes 22 partners from the Helmholtz Association, the Max Planck Society, the Leibniz Association, and universities, such as Goethe University[4].\nThe consortium’s work focuses on addressing the fundamental constituents of matter and their interactions, as well as their role in the development of the largest structures in the universe - stars and galaxies. The achievements of PUNCH science range from the discovery of the Higgs boson, the installation of a 1 cubic kilometre particle detector for neutrino detection in the Antarctic ice, the detection of the quark-gluon plasma in heavy-ion collisions, to the first picture ever of the black hole at the heart of the Milky Way[1][5].\nPUNCH4NFDI also offers services for the efficient scientific exploitation of research data. In doing so, it collaborates closely with its European and international partners at the forefront of research in scientific data management[1].\nThe consortium organizes various events and workshops, such as the “ErUM-Data-Hub: Workshop on Inverse Problems” and “Radio 2023”. These events are aimed at fostering collaboration and knowledge exchange within the scientific community[2].\nIn terms of governance, an Executive Board with representatives from the PUNCH community oversees PUNCH4NFDI. A Management Board, including task area and work package leaders, coordinates and monitors progress and resource allocation/usage. An external Science Advisory Committee of physicists and computer scientists provides advice on ongoing and future developments. An elected User Committee ensures feedback and input from the user side regarding the scientific directions of the project and the services offered. An Infrastructure Control Board is responsible for synchronizing the top-level requirements and deliverables with the national and international data providers[7].\n\n\n\nThe National Research Data Infrastructure (NFDI) is a German initiative designed to systematically manage scientific and research data, provide long-term data storage, backup and accessibility, and network the data both nationally and internationally. The NFDI is developed step by step and science-oriented with services available to researchers from various disciplines, institutions, and federal states.\nWithin the NFDI, there are specific sections where association members work together to develop cross-consortium standards, metadata standards, and formats. These sections are legally dependent departments of the NFDI Association, and they work on cross-sectional topics across the boundaries of the consortia. The establishment of these sections is a strategy-led process initiated by the NFDI Directorate and the NFDI Consortium Assembly.\nFive sections have been established:\n\nCommon Infrastructures (section-infra)\nEthical, Legal and Social Aspects (section-ELSA)\n(Meta)data, Terminologies, Provenance (section-metadata)\nTraining & Education (section-edutrain)\nIndustry Engagement (section-industry) [1]\n\nEach section focuses on different aspects of research data management. For instance, the (Meta)data, Terminologies, Provenance section supports processes of the NFDI consortia in jointly developing and establishing common data and metadata standards as a foundation for effective reuse of research data in accordance with FAIR principles. This section also addresses issues of (meta)data harmonization, discoverability of data, general data and metadata standards, format conversions, and persistent identifier systems.\nThe Training & Education section, on the other hand, provides an overview of various training materials and further training measures/events in research data management in all disciplines in German-speaking countries.\nThe NFDI is governed by a Board of Trustees, a Scientific Senate, and a Directorate. The Board of Trustees is the administrative-strategic supervisory body of the NFDI Association. The Scientific Senate is the central strategic body of the NFDI, advising on all fundamental matters relating to the ongoing development of the national research data infrastructure. The Directorate, located in Karlsruhe, consists of a full-time director and a managing office. The Directorate coordinates the work and activities of NFDI consortia on cross-disciplinary topics."
  },
  {
    "objectID": "Networking.html#erum",
    "href": "Networking.html#erum",
    "title": "Networking and Consortium",
    "section": "",
    "text": "ErUM-Data-Hub, funded by the Federal Ministry of Education and Research (BMBF), is a central networking and transfer office for the digital transformation in the exploration of universe and matter (ErUM). The main aims of the ErUM-Data-Hub are to network ErUM communities, identify and exploit synergies in ErUM-Data-projects in the field of digitalization, communicate research results to target groups, and facilitate knowledge and innovation transfer. Additionally, the ErUM-Data-Hub provides a diversified further education offer in the field of digital competence[1].\nErUM, which stands for “Erforschung von Universum und Materie,” involves about 20,000 scientists in Germany who are engaged in the exploration of the universe and matter using large-scale equipment. German ErUM research is organized in 8 ErUM committees, which coordinate and represent the interests of the different research areas[1].\nAs part of the ErUM-Data Plan of Action, the ErUM Data Hub pursues the overarching goal of digital transformation in fundamental ErUM-Pro research. In close coordination with the ErUM communities, the Hub implements the strategic measures of the ErUM Data Action Plan[1].\nThe ErUM-Data-Hub also organizes various events and workshops, such as the “Deep Learning School ‘Basic Concepts’” aimed at deep-learning starters from all ErUM communities who have a basic knowledge of physics[4].\nIn terms of research, progress in fundamental research on universe and matter (ErUM) is made by studying structures at smaller and smaller scales. The high resolution of modern instruments in particle, hadron and nuclear, and astroparticle physics results in huge amounts of research data. To address the challenge of increasing data rates and volumes, 17 partners from universities and research centers joined forces in the BMBF funded ErUM Data pilot project Innovative Digital Technologies for Research on Universe and Matter[2]."
  },
  {
    "objectID": "Networking.html#helmholtz",
    "href": "Networking.html#helmholtz",
    "title": "Networking and Consortium",
    "section": "",
    "text": "Helmholtz DMA, or Data Management and Analysis, is a research initiative within the Helmholtz-Zentrum Dresden-Rossendorf (HZDR). The primary objective of Helmholtz DMA is the implementation of a comprehensive digitization strategy for the Research Field MATTER. This involves the development, application, provision, and integration of innovative digital solutions for handling and analyzing the extreme volumes and rates of complex data coming from machines, experiments, and simulations. The initiative also focuses on the research and application of artificial intelligence for knowledge extraction from experiments and simulations, and the application of frontier technologies such as exascale computing and quantum computing to model complex states of matter[1][4].\nHelmholtz DMA is strongly aligned with the overall activities of the topic, with strong integration with the activities in the topic “Accelerator Research and Development” and the topic “Matter – Dynamics, Mechanisms and Control” in the program MML[1]. The DMA initiative also creates synergies in Matter between Helmholtz Centers & Facilities, Research Programs (MU,MML,MT), and Science Domains. It develops open, shareable solutions for Matter, including open source, modular solutions, open standards, professional, industry-grade software development, and F.A.I.R. Data[4].\nIn addition to its research activities, Helmholtz DMA also organizes various events and workshops, such as the “Research Data Management - Experts Workshop. Synergies between NFDI (including DAPHNE4NFDI), ErUM-Data and Helmholtz-DMA” that took place in Bonn in March 2023[3].\n\n\n\nThe Helmholtz Metadata Collaboration (HMC) is an initiative within the Helmholtz Association that aims to promote the qualitative enrichment of research data through metadata. HMC develops and implements novel concepts and technologies for sustainable handling of research data through high-quality metadata. Its main goal is to make the depth and breadth of research data produced by Helmholtz Centres findable, accessible, interoperable, and reusable (FAIR) for the whole science community[1].\nHMC is organized into several hubs, each focusing on a specific research field. These hubs pool scientific expertise on metadata and provide advice and infrastructure services for storing, reusing, and exchanging metadata. The work of HMC is always embedded in the national and international context, and it aims to keep research data “alive” by making it accessible and usable to interested parties in various disciplines[6]."
  },
  {
    "objectID": "Networking.html#data-project-with-rdm",
    "href": "Networking.html#data-project-with-rdm",
    "title": "Networking and Consortium",
    "section": "",
    "text": "The B3D project is an interdisciplinary collaboration between universities and research institutions in North Rhine-Westphalia (NRW), Germany. It aims to create a prosperous network of experts in astrophysics and data science. The project uses powerful analysis and visualization techniques, sustainable technology, and infrastructure to develop solutions for the upcoming challenges of the next generation of astronomical observatories. This is done in close collaboration with other leading academic and industrial partners[2].\nOne of the main goals of the B3D project is to generate synergies between different areas of research and to provide a joint research-oriented training program in future technologies that can be applied to astronomical research and beyond[2]. The project also aims to develop new measures for the qualification of young scientists[3].\nA central component of the B3D project is the development of innovative traversers using artificial intelligence and machine learning. These tools are used to systematically manage and analyze the immense flood of data to ensure better understanding[3].\nIn terms of Research Data Management (RDM), the B3D project is a prime example of how RDM principles can be applied to handle and analyze large volumes of data in a scientific context. Effective RDM is a key component of research integrity and reproducible research, and its importance is increasingly recognized in data-intensive fields like radio astronomy[6].\n\n\n\nSFB 1601 is a collaborative research center that focuses on the study of the interstellar medium, star formation, and the evolution of galaxies. The project Inf - SFB 1601: RDM services and research product exchange is a part of this collaborative research center and aims to establish an interconnected infrastructure that ensures the documentation, preservation, and quality assessment of data and procedures. This infrastructure allows project partners to select and use the RDM services most suitable for their own needs, exchange research products (data, procedures, code, etc.) efficiently and transparently, and make all relevant scientific data accessible within the CRC and to the astronomical community as a whole.\nThe CRC research is based on four pillars: instrumentation, observations, 3D simulations, and laboratory astrophysics. This implies data and workflows of different types and specifications, which require different research data management (RDM) and research software management (RSM) solutions. The project aims to cover the gaps in RDM within the four pillars that were identified in the needs assessment and classified into three categories:\n\nDocumentation & legal aspects,\nRDM/RSM support along the data-life cycle,\nSkills development & Cooperation."
  },
  {
    "objectID": "Stakeholders.html",
    "href": "Stakeholders.html",
    "title": "RDM 4 Astro",
    "section": "",
    "text": "Data Management Stakeholders\n\nThe research lifecycle from the Data Documentation Initiative (DDI) provides a framework for understanding where specific data management practices fall.\nDuring the discovery and planning phase, researchers need to determine what type and format of data they are going to collect, consider ethical issues, and identify potential reusers of the project data. They should also determine the possible costs surrounding data management and identify appropriate data repositories.\nDuring the data collection phase, researchers should follow data management best practices, including file organization, backup and storage strategies, and quality assurance protocols. They should also consider access controls and data security.\nDuring the preparation and data analysis phase, researchers may need to clean, manipulate or process the raw data, and should document any changes to the raw data and create a master version to be analyzed and eventually archived. They should also document analysis procedures, such as modifications to the data, the model used, the code used to run the analysis, and hardware and software specifications.\nDuring the publication and sharing phase, researchers should prepare their data files and other research materials necessary to interpret and reuse the data in the future. They should consult with information professionals or data repository staff and ensure that their data management meets all of the needs and requirements of the repository.\nTrusted repositories perform functions to ensure the long-term management of data, including ensuring the integrity of the data, protecting against data loss, and providing access to data."
  },
  {
    "objectID": "Training.html",
    "href": "Training.html",
    "title": "RDM 4 Astro",
    "section": "",
    "text": "PUNCH4NFDI, the NFDI consortium of particle, astro-, astroparticle, hadron, and nuclear physics, offers various types of Research Data Management (RDM) training, including workshops, online courses, and documentation.\nWorkshops: PUNCH4NFDI organizes workshops on various topics. For instance, in June 2023, three workshops were organized in collaboration with PUNCH4NFDI, covering topics such as ILDG, SciTrace, and FPGAs. They also held their first-ever workshop on use cases in the NFDI, presented by Physical Sciences and organized by NFDI4Chem, NFDI4Cat, DAPHNE4NFDI, and FAIRmat.\nOnline Courses: PUNCH4NFDI also offers online courses as part of their RDM training. This is mentioned in their consortium proposal, where they discuss the mitigation of certain challenges through the offering of online courses.\nDocumentation: PUNCH4NFDI provides documentation and guides as part of their RDM training. This is evident from their consortium proposal, where they discuss the provision of user and group-specific documentation.\nIn addition to these, PUNCH4NFDI also collaborates with its European and international partners at the forefront of research in scientific data management, which could provide further opportunities for RDM training."
  },
  {
    "objectID": "Training.html#punch4nfdi",
    "href": "Training.html#punch4nfdi",
    "title": "RDM 4 Astro",
    "section": "",
    "text": "PUNCH4NFDI, the NFDI consortium of particle, astro-, astroparticle, hadron, and nuclear physics, offers various types of Research Data Management (RDM) training, including workshops, online courses, and documentation.\nWorkshops: PUNCH4NFDI organizes workshops on various topics. For instance, in June 2023, three workshops were organized in collaboration with PUNCH4NFDI, covering topics such as ILDG, SciTrace, and FPGAs. They also held their first-ever workshop on use cases in the NFDI, presented by Physical Sciences and organized by NFDI4Chem, NFDI4Cat, DAPHNE4NFDI, and FAIRmat.\nOnline Courses: PUNCH4NFDI also offers online courses as part of their RDM training. This is mentioned in their consortium proposal, where they discuss the mitigation of certain challenges through the offering of online courses.\nDocumentation: PUNCH4NFDI provides documentation and guides as part of their RDM training. This is evident from their consortium proposal, where they discuss the provision of user and group-specific documentation.\nIn addition to these, PUNCH4NFDI also collaborates with its European and international partners at the forefront of research in scientific data management, which could provide further opportunities for RDM training."
  },
  {
    "objectID": "Training.html#c³rdm",
    "href": "Training.html#c³rdm",
    "title": "RDM 4 Astro",
    "section": "C³RDM",
    "text": "C³RDM\nThe Cologne Competence Center for Research Data Management (C³RDM) offers comprehensive support and advice on research data management (RDM) throughout all phases of a research project. This includes the development of a data management plan as part of the application process, support in finding suitable repositories, technical storage options, and the publication of research data. In addition to workshops and training courses, C³RDM also offers personal and individual consultations in institutes, subject groups, and working groups.\nFor online courses and resources, there are several options available. The Research Data Management Librarian Academy (RDMLA) is a free online professional development program that focuses on the essential knowledge and skills needed to collaborate effectively with researchers on data management. The curriculum covers topics such as the foundations of research data management (RDM), research culture, advocating and marketing for RDM services in libraries, project management, and an overview of research data management tools.\nhttps://fdm.uni-koeln.de/en/c3rdm-team"
  },
  {
    "objectID": "Training.html#mantra",
    "href": "Training.html#mantra",
    "title": "RDM 4 Astro",
    "section": "MANTRA",
    "text": "MANTRA\nThe MANTRA Research Data Management (RDM) Training is a free online course designed for researchers or others who manage digital data as part of their research projects. The course is hosted by the University of Edinburgh and covers various aspects of RDM, including data management planning, organizing data, file formats and transformation, documentation, metadata, citation, storage and security, protecting sensitive data, sharing, preservation, and licensing.\nThe course consists of seven interactive modules, along with a set of data handling tutorials that provide practical exercises using software analysis packages such as SPSS, R, ArcGIS, and NVivo. \nEach module takes around 20 minutes to complete, allowing learners to proceed at their own pace. The course is designed to be accessible to researchers from various disciplines and provides training on data management following the key stages of any research project.\nTo access the MANTRA RDM training, visit the University of Edinburgh’s MANTRA website at https://mantra.ed.ac.uk."
  },
  {
    "objectID": "DataPolicies.html",
    "href": "DataPolicies.html",
    "title": "Data Policies in Astrophysics",
    "section": "",
    "text": "Data policies in astrophysics have been developed to ensure the responsible management, sharing, and utilization of astronomical data. These policies establish guidelines and practices that govern various aspects of data handling, including collection, storage, access, and dissemination. They play a crucial role in promoting transparency, collaboration, and the advancement of scientific research in the field of astrophysics.\nThe formulation of these policies is based on a comprehensive evaluation of existing German research data (RD) policies, a comparison with international recommendations for RD policy development, as well as expert interviews conducted with German universities and a technical college. This process has allowed for the subdivision of key questions and text modules according to the RD policy scheme.\nThe key questions posed during the policy development process draw insights from the evaluation of German RD policies, ensuring alignment with best practices. Additionally, international recommendations for RD policy creation have been taken into account to ensure the policies are robust and globally relevant. Expert interviews with German experts from universities have provided valuable input and perspectives.\nBy incorporating these key questions and text modules into the data policies, the astrophysics community can establish a comprehensive and effective framework for managing research data. This will facilitate data transparency, collaboration, and adherence to best practices, ultimately driving advancements in the field of astrophysics.\n\n\n@article{hiemenz2018empfehlungen, title={Empfehlungen zur Erstellung institutioneller Forschungsdaten-Policies. Das Forschungsdaten-Policy-Kit als generischer Baukasten mit Leitfragen und Textbausteinen f{“u}r Hochschulen in Deutschland}, author={Hiemenz, Bea and Kuberek, Monika}, year={2018} }\n1 Preamble\n\n1.1 Aim of the institution, importance of the RDM\n1.2 Standards and principles relating to RDM\n\n2 Scope\n\n2.1 Range\n2.2 Relationship to legal requirements/ contracts with third parties\n\n3 Legal/Ethical Aspects\n\n3.1 Ownership of rights/rights of use\n3.2 Data protection\n3.3 Transfer of rights\n\n4 Handling Of Research Data\n\n4.1 Basic principles\n4.2 Data selection\n4.3 Access/Licensing\n4.4 Storage time/storage duration\n\n5 Responsibilities - Researchers\n\n5.1 Researchers\n5.2 Principles RDM\n5.3 Data Management Plan (DMP)\n5.4 Project Regulations\n\n6 Responsibilities - Institution\n\n6.1 Infrastructure\n6.2 Consulting/Education/Training\n6.3 Cooperation\n\n7 Validity\n\n7.1 Validity/Review\n7.2 Contact\n\n8 Annex/Glossary (definitions, references)\n\n8.1 Definitions\n8.2 Reference to related documents"
  },
  {
    "objectID": "DataPolicies.html#content-of-the-data-policies",
    "href": "DataPolicies.html#content-of-the-data-policies",
    "title": "Data Policies in Astrophysics",
    "section": "",
    "text": "@article{hiemenz2018empfehlungen, title={Empfehlungen zur Erstellung institutioneller Forschungsdaten-Policies. Das Forschungsdaten-Policy-Kit als generischer Baukasten mit Leitfragen und Textbausteinen f{“u}r Hochschulen in Deutschland}, author={Hiemenz, Bea and Kuberek, Monika}, year={2018} }\n1 Preamble\n\n1.1 Aim of the institution, importance of the RDM\n1.2 Standards and principles relating to RDM\n\n2 Scope\n\n2.1 Range\n2.2 Relationship to legal requirements/ contracts with third parties\n\n3 Legal/Ethical Aspects\n\n3.1 Ownership of rights/rights of use\n3.2 Data protection\n3.3 Transfer of rights\n\n4 Handling Of Research Data\n\n4.1 Basic principles\n4.2 Data selection\n4.3 Access/Licensing\n4.4 Storage time/storage duration\n\n5 Responsibilities - Researchers\n\n5.1 Researchers\n5.2 Principles RDM\n5.3 Data Management Plan (DMP)\n5.4 Project Regulations\n\n6 Responsibilities - Institution\n\n6.1 Infrastructure\n6.2 Consulting/Education/Training\n6.3 Cooperation\n\n7 Validity\n\n7.1 Validity/Review\n7.2 Contact\n\n8 Annex/Glossary (definitions, references)\n\n8.1 Definitions\n8.2 Reference to related documents"
  }
]